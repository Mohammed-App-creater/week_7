{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Exploratory Data Analysis and Data Preprocessing\n",
    "\n",
    "**Objective**: Load, explore, filter, clean, and preprocess the CFPB complaints dataset for downstream NLP tasks.\n",
    "\n",
    "**Dataset**: Consumer Financial Protection Bureau (CFPB) Complaints\n",
    "\n",
    "**Author**: Data & AI Engineer\n",
    "\n",
    "**Date**: 2026-01-06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "Load the full CFPB complaints dataset from the `data/raw/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file path\n",
    "data_path = Path('../data/raw/')\n",
    "csv_files = list(data_path.glob('*.csv'))\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV file(s) in data/raw/:\")\n",
    "for f in csv_files:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "# Load the dataset (assuming the first CSV is the complaints dataset)\n",
    "if csv_files:\n",
    "    df_raw = pd.read_csv(csv_files[0], low_memory=False)\n",
    "    print(f\"\\nDataset loaded: {csv_files[0].name}\")\n",
    "    print(f\"Shape: {df_raw.shape}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No CSV files found in data/raw/. Please add the CFPB complaints dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initial Data Exploration\n",
    "\n",
    "Examine the structure, columns, and basic statistics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows:\")\n",
    "display(df_raw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "print(\"Dataset Information:\")\n",
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': df_raw.columns,\n",
    "    'Missing_Count': df_raw.isnull().sum().values,\n",
    "    'Missing_Percentage': (df_raw.isnull().sum().values / len(df_raw) * 100).round(2)\n",
    "})\n",
    "display(missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the complaint narrative column\n",
    "# Common column names: 'Consumer complaint narrative', 'Complaint', 'Consumer Complaint', etc.\n",
    "narrative_cols = [col for col in df_raw.columns if 'narrative' in col.lower() or 'complaint' in col.lower()]\n",
    "print(f\"Potential narrative columns: {narrative_cols}\")\n",
    "\n",
    "# Identify the product column\n",
    "product_cols = [col for col in df_raw.columns if 'product' in col.lower()]\n",
    "print(f\"Potential product columns: {product_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 4.1 Distribution of Complaints by Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the product column is named 'Product' (adjust if different)\n",
    "product_col = 'Product' if 'Product' in df_raw.columns else product_cols[0] if product_cols else None\n",
    "\n",
    "if product_col:\n",
    "    # Count complaints by product\n",
    "    product_counts = df_raw[product_col].value_counts()\n",
    "    \n",
    "    print(f\"Total unique products: {len(product_counts)}\")\n",
    "    print(f\"\\nTop 15 Products by Complaint Count:\")\n",
    "    display(product_counts.head(15).to_frame(name='Count'))\n",
    "    \n",
    "    # Visualize top 15 products\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    product_counts.head(15).plot(kind='barh', color='steelblue')\n",
    "    plt.xlabel('Number of Complaints', fontsize=12)\n",
    "    plt.ylabel('Product', fontsize=12)\n",
    "    plt.title('Top 15 Products by Complaint Count', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Product column not found. Please verify the dataset structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Count of Complaints With vs Without Consumer Complaint Narratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the narrative column is named 'Consumer complaint narrative' (adjust if different)\n",
    "narrative_col = 'Consumer complaint narrative' if 'Consumer complaint narrative' in df_raw.columns else narrative_cols[0] if narrative_cols else None\n",
    "\n",
    "if narrative_col:\n",
    "    # Count narratives\n",
    "    has_narrative = df_raw[narrative_col].notna().sum()\n",
    "    no_narrative = df_raw[narrative_col].isna().sum()\n",
    "    total = len(df_raw)\n",
    "    \n",
    "    print(f\"Total Complaints: {total:,}\")\n",
    "    print(f\"With Narrative: {has_narrative:,} ({has_narrative/total*100:.2f}%)\")\n",
    "    print(f\"Without Narrative: {no_narrative:,} ({no_narrative/total*100:.2f}%)\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Bar chart\n",
    "    categories = ['With Narrative', 'Without Narrative']\n",
    "    counts = [has_narrative, no_narrative]\n",
    "    colors = ['#2ecc71', '#e74c3c']\n",
    "    \n",
    "    ax1.bar(categories, counts, color=colors, alpha=0.8)\n",
    "    ax1.set_ylabel('Number of Complaints', fontsize=12)\n",
    "    ax1.set_title('Complaints With vs Without Narratives', fontsize=14, fontweight='bold')\n",
    "    ax1.ticklabel_format(style='plain', axis='y')\n",
    "    \n",
    "    for i, v in enumerate(counts):\n",
    "        ax1.text(i, v + total*0.01, f'{v:,}\\n({v/total*100:.1f}%)', ha='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Pie chart\n",
    "    ax2.pie(counts, labels=categories, autopct='%1.1f%%', colors=colors, startangle=90, textprops={'fontsize': 11})\n",
    "    ax2.set_title('Proportion of Complaints', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Narrative column not found. Please verify the dataset structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Word Count Distribution of Complaint Narratives\n",
    "\n",
    "Analyze the length distribution of complaint narratives to understand text complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if narrative_col:\n",
    "    # Filter non-null narratives\n",
    "    narratives = df_raw[df_raw[narrative_col].notna()][narrative_col]\n",
    "    \n",
    "    # Calculate word counts\n",
    "    word_counts = narratives.apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"Word Count Statistics:\")\n",
    "    print(f\"Mean: {word_counts.mean():.2f}\")\n",
    "    print(f\"Median: {word_counts.median():.2f}\")\n",
    "    print(f\"Std Dev: {word_counts.std():.2f}\")\n",
    "    print(f\"Min: {word_counts.min()}\")\n",
    "    print(f\"Max: {word_counts.max()}\")\n",
    "    print(f\"25th Percentile: {word_counts.quantile(0.25):.2f}\")\n",
    "    print(f\"75th Percentile: {word_counts.quantile(0.75):.2f}\")\n",
    "    \n",
    "    # Visualize distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Histogram\n",
    "    axes[0].hist(word_counts, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    axes[0].axvline(word_counts.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {word_counts.mean():.0f}')\n",
    "    axes[0].axvline(word_counts.median(), color='green', linestyle='--', linewidth=2, label=f'Median: {word_counts.median():.0f}')\n",
    "    axes[0].set_xlabel('Word Count', fontsize=12)\n",
    "    axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[0].set_title('Distribution of Word Counts in Complaint Narratives', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Box plot\n",
    "    axes[1].boxplot(word_counts, vert=True, patch_artist=True, \n",
    "                    boxprops=dict(facecolor='lightblue', color='blue'),\n",
    "                    medianprops=dict(color='red', linewidth=2),\n",
    "                    whiskerprops=dict(color='blue'),\n",
    "                    capprops=dict(color='blue'))\n",
    "    axes[1].set_ylabel('Word Count', fontsize=12)\n",
    "    axes[1].set_title('Box Plot of Word Counts', fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional analysis: narratives with very few words\n",
    "    short_narratives = (word_counts < 10).sum()\n",
    "    print(f\"\\nNarratives with < 10 words: {short_narratives:,} ({short_narratives/len(word_counts)*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"Narrative column not found. Cannot analyze word counts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Filtering\n",
    "\n",
    "Filter the dataset to include ONLY the following products:\n",
    "- Credit card\n",
    "- Personal loan\n",
    "- Savings account\n",
    "- Money transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if product_col:\n",
    "    # Define target products (case-insensitive matching)\n",
    "    target_products = ['Credit card', 'Personal loan', 'Savings account', 'Money transfer']\n",
    "    \n",
    "    # Check exact matches first\n",
    "    print(\"Checking for exact product matches...\")\n",
    "    exact_matches = df_raw[product_col].isin(target_products)\n",
    "    print(f\"Exact matches found: {exact_matches.sum()}\")\n",
    "    \n",
    "    # If no exact matches, try case-insensitive partial matching\n",
    "    if exact_matches.sum() == 0:\n",
    "        print(\"\\nNo exact matches. Attempting case-insensitive partial matching...\")\n",
    "        print(\"\\nAvailable products in dataset:\")\n",
    "        print(df_raw[product_col].unique()[:20])  # Show first 20 products\n",
    "        \n",
    "        # Create a filter using case-insensitive partial matching\n",
    "        filter_mask = df_raw[product_col].str.lower().str.contains(\n",
    "            '|'.join([p.lower() for p in target_products]), \n",
    "            na=False, \n",
    "            regex=True\n",
    "        )\n",
    "    else:\n",
    "        filter_mask = exact_matches\n",
    "    \n",
    "    # Apply filter\n",
    "    df_filtered = df_raw[filter_mask].copy()\n",
    "    \n",
    "    print(f\"\\nOriginal dataset size: {len(df_raw):,}\")\n",
    "    print(f\"Filtered dataset size: {len(df_filtered):,}\")\n",
    "    print(f\"Reduction: {(1 - len(df_filtered)/len(df_raw))*100:.2f}%\")\n",
    "    \n",
    "    # Show distribution of filtered products\n",
    "    print(f\"\\nProduct distribution after filtering:\")\n",
    "    display(df_filtered[product_col].value_counts().to_frame(name='Count'))\n",
    "else:\n",
    "    print(\"Product column not found. Cannot filter by product.\")\n",
    "    df_filtered = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Remove Rows with Empty or Null Narratives\n",
    "\n",
    "Remove all rows where the consumer complaint narrative is empty or null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if narrative_col:\n",
    "    print(f\"Before removing null narratives: {len(df_filtered):,} rows\")\n",
    "    \n",
    "    # Remove null and empty narratives\n",
    "    df_filtered = df_filtered[df_filtered[narrative_col].notna()].copy()\n",
    "    df_filtered = df_filtered[df_filtered[narrative_col].str.strip() != ''].copy()\n",
    "    \n",
    "    print(f\"After removing null narratives: {len(df_filtered):,} rows\")\n",
    "    print(f\"Rows removed: {len(df_raw[filter_mask]) - len(df_filtered):,}\")\n",
    "else:\n",
    "    print(\"Narrative column not found. Cannot remove null narratives.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Text Cleaning and Preprocessing\n",
    "\n",
    "Clean the complaint narratives by:\n",
    "1. Converting to lowercase\n",
    "2. Removing special characters and extra whitespace\n",
    "3. Removing common boilerplate phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean complaint narrative text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw complaint text\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    \n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove boilerplate phrases (case-insensitive)\n",
    "    boilerplate_phrases = [\n",
    "        r'i am writing to file a complaint',\n",
    "        r'i am filing this complaint',\n",
    "        r'i would like to file a complaint',\n",
    "        r'i wish to file a complaint',\n",
    "        r'this is a complaint regarding',\n",
    "        r'i am submitting this complaint',\n",
    "        r'dear sir or madam',\n",
    "        r'to whom it may concern',\n",
    "        r'dear cfpb',\n",
    "    ]\n",
    "    \n",
    "    for phrase in boilerplate_phrases:\n",
    "        text = re.sub(phrase, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove special characters (keep letters, numbers, and spaces)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test the cleaning function\n",
    "sample_text = \"I am writing to file a complaint about my Credit Card!! The company charged me $500.00 (unauthorized).\"\n",
    "print(\"Original text:\")\n",
    "print(sample_text)\n",
    "print(\"\\nCleaned text:\")\n",
    "print(clean_text(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if narrative_col:\n",
    "    # Create a new column for cleaned narratives\n",
    "    print(\"Cleaning complaint narratives...\")\n",
    "    df_filtered['cleaned_narrative'] = df_filtered[narrative_col].apply(clean_text)\n",
    "    \n",
    "    # Remove rows where cleaned narrative is empty\n",
    "    initial_count = len(df_filtered)\n",
    "    df_filtered = df_filtered[df_filtered['cleaned_narrative'].str.strip() != ''].copy()\n",
    "    final_count = len(df_filtered)\n",
    "    \n",
    "    print(f\"Rows removed after cleaning (empty after processing): {initial_count - final_count:,}\")\n",
    "    print(f\"Final dataset size: {final_count:,}\")\n",
    "    \n",
    "    # Show examples\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SAMPLE BEFORE AND AFTER CLEANING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i in range(min(3, len(df_filtered))):\n",
    "        idx = df_filtered.index[i]\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(\"ORIGINAL:\")\n",
    "        print(df_filtered.loc[idx, narrative_col][:300] + \"...\" if len(df_filtered.loc[idx, narrative_col]) > 300 else df_filtered.loc[idx, narrative_col])\n",
    "        print(\"\\nCLEANED:\")\n",
    "        print(df_filtered.loc[idx, 'cleaned_narrative'][:300] + \"...\" if len(df_filtered.loc[idx, 'cleaned_narrative']) > 300 else df_filtered.loc[idx, 'cleaned_narrative'])\n",
    "else:\n",
    "    print(\"Narrative column not found. Cannot clean narratives.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Dataset Statistics\n",
    "\n",
    "Summary statistics of the cleaned and filtered dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FINAL DATASET SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total rows: {len(df_filtered):,}\")\n",
    "print(f\"Total columns: {len(df_filtered.columns)}\")\n",
    "print(f\"\\nColumns: {list(df_filtered.columns)}\")\n",
    "\n",
    "if product_col:\n",
    "    print(f\"\\nProduct Distribution:\")\n",
    "    display(df_filtered[product_col].value_counts().to_frame(name='Count'))\n",
    "\n",
    "if 'cleaned_narrative' in df_filtered.columns:\n",
    "    # Word count statistics for cleaned narratives\n",
    "    cleaned_word_counts = df_filtered['cleaned_narrative'].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    print(f\"\\nCleaned Narrative Word Count Statistics:\")\n",
    "    print(f\"Mean: {cleaned_word_counts.mean():.2f}\")\n",
    "    print(f\"Median: {cleaned_word_counts.median():.2f}\")\n",
    "    print(f\"Std Dev: {cleaned_word_counts.std():.2f}\")\n",
    "    print(f\"Min: {cleaned_word_counts.min()}\")\n",
    "    print(f\"Max: {cleaned_word_counts.max()}\")\n",
    "\n",
    "print(f\"\\nMemory usage: {df_filtered.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Cleaned Dataset\n",
    "\n",
    "Save the cleaned and filtered dataset to `data/filtered_complaints.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output path\n",
    "output_path = Path('../data/filtered_complaints.csv')\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "df_filtered.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to: {output_path}\")\n",
    "print(f\"File size: {output_path.stat().st_size / 1024**2:.2f} MB\")\n",
    "print(f\"\\nDataset shape: {df_filtered.shape}\")\n",
    "print(f\"Columns saved: {list(df_filtered.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Verification\n",
    "\n",
    "Verify the saved dataset can be loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved dataset\n",
    "df_verify = pd.read_csv(output_path)\n",
    "\n",
    "print(\"Verification:\")\n",
    "print(f\"Loaded shape: {df_verify.shape}\")\n",
    "print(f\"Original shape: {df_filtered.shape}\")\n",
    "print(f\"Match: {df_verify.shape == df_filtered.shape}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 3 rows of saved dataset:\")\n",
    "display(df_verify.head(3))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TASK 1 COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
